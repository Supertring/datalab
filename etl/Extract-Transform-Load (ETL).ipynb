{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82effeb4",
   "metadata": {},
   "source": [
    "# What is ETL?\n",
    "\n",
    "* __E__: Extract\n",
    "* __T__: Transform\n",
    "* __L__: Load\n",
    "\n",
    "A data integration process that combines data from multiple data sources into a single, transforms the data anc finally loads data into data warehouse system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b769acf",
   "metadata": {},
   "source": [
    "# __Extract__:\n",
    "\n",
    "* raw data is copied or exported from source locations to a staging area.\n",
    "* data extracted from various sources\n",
    "    * SQL/ NoSQL servers\n",
    "    * ERP systems\n",
    "    * flat files\n",
    "    * Emails\n",
    "    * web pages\n",
    "* __Identify Data Sources__:\n",
    "    * Find the source of data\n",
    "    * Includes database, spreadsheets, APIs, web services, logs, or other data repositories\n",
    "\n",
    "* __Define Extraction Methods__: \n",
    "    * Decide best approach to extract data from the source\n",
    "    * __FUll Extraction__: extract all the data in one time\n",
    "    * __Incremental extraction__: extract data incrementally, in batches\n",
    "    * __Partial extraction__: extract partial data\n",
    "    * __Real-time__: extract data in real time when needed in as per the requirenment\n",
    "\n",
    "* __Access Source Systems__:\n",
    "    * Access data using methods such as:\n",
    "    * SQL queries for relational databases\n",
    "    * API calls from web services\n",
    "    * CSV, EXCEL, JSON, XML file readings\n",
    "    * Extract information from log files\n",
    "    \n",
    "* __Retrieve Data__:\n",
    "    * Extract and ensure required fields and records as per your requirenments\n",
    "  \n",
    "* __Data Profiling__:\n",
    "    * Analyze extracted data\n",
    "    * Understand structure, data types, null values, unique values, patterns, quality issues\n",
    "    * data profiling helps idenfity issues early in the process\n",
    "  \n",
    "* __DAta validation__: \n",
    "    * verify extracted data meets expectations and defined rules\n",
    "    * check missing values, data formats inconsistencies, and any anomalies\n",
    "    \n",
    "* __DAta staging__: \n",
    "    * __Store the data in Staging area__\n",
    "    * This area acts as an storage space before the transformation phase\n",
    "    * Staging allows you to manipulate and validate data without affecting the raw source system/data\n",
    "  \n",
    "* __Metadata capture__: \n",
    "    * Document metadata related to extracted data, eg: source, time, method, tranformation etcs.\n",
    "* __Error handling and logging__: \n",
    "    * design mechanism to handle errors while data extractions, \n",
    "    * create logs about success and faliures, \n",
    "    * setup alerts and notifications for issues that occurs during the process\n",
    "* __Data security and compliance__:\n",
    "    * Ensure extractions is within security and regulations, \n",
    "    * apply encryption, authentication and access controls as required\n",
    "* __Testing and verifications__: \n",
    "    * test if the extraction mechanism extract accuracte and complete data, \n",
    "    * compare source and extracted data to verify consistency.\n",
    "    \n",
    "Extract phase is a foundation to the next phase of ETL process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da9c9f",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "* In the staging area, raw data undergoes data processing.\n",
    "* The data is transformed according to use cases.\n",
    "* Some task are as follows:\n",
    "    * __Data Cleaning__: remove or correct incorrect data, missing values, incosistencies\n",
    "    * __Data Enrichment__: Add new data to enhance the information in the dataset. This can be done by merging                            data from different sources, using external data, creating new variables\n",
    "    * __Data Filtering__: Find a subset of data as per specified criteria\n",
    "    * __Data Aggregation__: Summarize data by groups/categories, use functions like SUM, AVG, COUNT etc.\n",
    "    * __Data Transformation__: apply mathematical operations, eg: converting units, scaling, apply logarithms,                               normalization\n",
    "    * __Data Formatting__: Prepare the data to a specific format that is required for a target system.\n",
    "    * __Data joining/merging__: combine data from various sources based on common fields\n",
    "    * __Data duplication__: Removing duplicate records from the data\n",
    "    * __Data validation__: Check if data meets quality and consistency  as per the defined standards\n",
    "    * __DAta derivation__: create new data using calculations based on existing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4f03b",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "* The transformed data is transferred to target data warehouse from stagging storage.\n",
    "\n",
    "* __Target System Selection__:\n",
    "    * Setup best target system, as per your data storage and analysis.\n",
    "    * Can be relational db, NoSQL, data warehouse, data lake, disk or any other storage\n",
    "\n",
    "* __Desing Schema__:\n",
    "    * Design right schema to store the transformed data\n",
    "    * Eg: tables, fields, relationships\n",
    "   \n",
    "* __DAta Transformation__:\n",
    "    * some extra transformation can be done optionally.\n",
    "    * eg: aggregations, calculations etcs.\n",
    "\n",
    "* __Data Mapping__:\n",
    "    * Map the fields between source and target systems\n",
    "    * Ensure data types in the source match those in the target\n",
    " \n",
    "* __DAta loading Stragegies__:\n",
    "    * Chose right data loading method, \n",
    "    * As per factors such as data volume, frequencey of updates, system capabilites\n",
    "        * __Full Load__: Load all the transformed data in one run.\n",
    "        * __Incremental load__: load only new/recently modifined data since the last load\n",
    "        * __Delta load__: load changes/updates of the data\n",
    "        * __Append-only load__: load only new data without updating existing ones\n",
    "        * __Merge load__: Combine multiple data sources into a single target dataset\n",
    " \n",
    "* __Data loading Techniques__:\n",
    "    * __Bulk loading__: load huge volumes of data efficiently at a time\n",
    "    * __Inserts__: Insert new data into the target systems\n",
    "    * __Updates__: Update existing records if needed\n",
    "    * __Upserts__: Insert new records and update existing records if they exist\n",
    "    \n",
    "* __Data quality checks__:\n",
    "    * ensure data quality while loading data\n",
    "    * ensure the integrity of data loaded\n",
    "    * check integrity constraints, referential integrity, and other business rules\n",
    "    \n",
    "* __Error handling and logging__:\n",
    "    * Implement error handling mechanism, eg: data validation faliures/db errors\n",
    "    * save logs to keep track of load and issues that might come up\n",
    "   \n",
    "* __Indexing and optimization__:\n",
    "    * Design appropriate indexs on the target db to optimize query performance\n",
    "    * make use of clustering strategies for better data retrieval\n",
    "\n",
    "* __Metadata updates__:\n",
    "    * Save and update metadata information on loaded data\n",
    "    * eg: timestamps, load status, etcs\n",
    "\n",
    "* __Data validation and verification__:\n",
    "    * Ensure target loaded data in the target system mathces the transformed data\n",
    "    * ensure it meets the business rules and standards\n",
    "    \n",
    "* __Scheduling and Automation__:\n",
    "    * Make use of scheduling and automation methods as per the business needs and the eco systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab70f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
